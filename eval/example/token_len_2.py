# æ–‡ä»¶å: token_len_logger_final_v4.py (æœ€ç»ˆç‰ˆ - ä¿®æ­£æ•°æ®ç»“æ„å‡è®¾å’Œæ­£åˆ™)

import json
import argparse
import os
import re
import csv
import time
from tqdm import tqdm
from typing import List, Dict, Optional, DefaultDict
from collections import defaultdict
from transformers import AutoTokenizer

# =================== æ—¥å¿—è®°å½•è¾…åŠ©å‡½æ•° (ä¿®æ­£æ­£åˆ™) ===================
def parse_info(path: str, tokenizer_path: str, base_model_prefix: str, is_base_model_flag: bool) -> Optional[Dict[str, str]]:
    """
    ä»æ–‡ä»¶å’Œè·¯å¾„ä¸­è§£ææ‰€æœ‰å…ƒæ•°æ®ã€‚
    - ä¿®æ­£äº†æ­£åˆ™è¡¨è¾¾å¼ä»¥åŒ¹é… 'details_...' å’Œæ—¶é—´æˆ³ã€‚
    """
    filename = os.path.basename(path)
    
    # æ ¸å¿ƒä¿®æ­£ï¼šæ›´æ–°æ­£åˆ™è¡¨è¾¾å¼ä»¥åŒ¹é…æ‚¨çš„æ–‡ä»¶åæ ¼å¼
    # details_  <suite>  | <task> :<subset>| <shots> _<timestamp>_results.jsonl
    match_lighteval = re.search(r"details_([^|]+)\|([^|:]+):?([^|]*)\|(\d+)_.*?_results\.jsonl$", filename)
    
    if not match_lighteval:
        print(f"  [è§£æè­¦å‘Š] æ–‡ä»¶å '{filename}' ä¸ç¬¦åˆé¢„æœŸçš„ 'details_suite|task|..._results.jsonl' æ¨¡å¼ã€‚")
        return None
        
    suite, task_main, task_sub, _ = match_lighteval.groups()
    dataset_name = f"{suite}-{task_main}"
    if task_sub:
        dataset_name += f"-{task_sub}"
            
    clean_path = tokenizer_path.strip('/')
    
    if is_base_model_flag:
        step = "origin"
        model_part = os.path.basename(clean_path)
    else:
        head, tail = os.path.split(clean_path)
        step = "origin" 
        model_part = tail
        match_step = re.search(r"(?:step_|checkpoint-)(\d+)", tail) # å…¼å®¹ step_ å’Œ checkpoint-
        if match_step:
            step = match_step.group(1)
            model_part = os.path.basename(head)
    
    final_model_name = f"{base_model_prefix}-{model_part}"
            
    return {
        "experiment": "lighteval_run", 
        "model": final_model_name, 
        "step": step,
        "dataset": dataset_name
    }

def log_result_to_csv(log_file: str, file_info: Dict, metric_name: str, metric_value: float):
    """å°†ç»“æœï¼ˆåŒ…å«stepï¼‰è®°å½•åˆ°æŒ‡å®šçš„CSVæ–‡ä»¶ä¸­ã€‚"""
    try:
        log_dir = os.path.dirname(log_file)
        if log_dir: os.makedirs(log_dir, exist_ok=True)
        file_exists = os.path.isfile(log_file)
        with open(log_file, 'a', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            if not file_exists or os.path.getsize(log_file) == 0:
                writer.writerow(['timestamp', 'experiment', 'model', 'step', 'dataset', 'metric_name', 'metric_value'])
            
            writer.writerow([
                time.strftime('%Y-%m-%d %H:%M:%S'), 
                file_info['experiment'], 
                file_info['model'], 
                file_info['step'],
                file_info['dataset'], 
                metric_name, 
                f"{metric_value:.4f}"
            ])
    except Exception as e:
        print(f"  [ä¸¥é‡é”™è¯¯] æ— æ³•å†™å…¥æ—¥å¿—æ–‡ä»¶ {log_file}: {e}")

# =================== æ ¸å¿ƒè®¡ç®—é€»è¾‘ (å®Œå…¨é‡æ„) ===================

def calculate_and_log_token_stats(data_path: str, tokenizer, args: argparse.Namespace):
    """
    é‡æ„æ­¤å‡½æ•°ä»¥æ­£ç¡®å¤„ç†lightevalçš„detailsæ–‡ä»¶ç»“æ„ã€‚
    æ–‡ä»¶ä¸­çš„æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªé—®é¢˜ã€‚
    """
    # ç»“æ„: responses_by_run[run_index] = [token_len_q1, token_len_q2, ...]
    responses_by_run: DefaultDict[int, List[int]] = defaultdict(list)
    total_token_count = 0
    total_responses = 0
    num_questions = 0

    if not os.path.exists(data_path):
        print(f"  [é”™è¯¯] æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return

    # === ç¬¬ä¸€æ­¥: é€è¡Œè¯»å–æ–‡ä»¶ï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ªé—®é¢˜ ===
    with open(data_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
        num_questions = len(lines)
        if num_questions == 0:
            print("  [ä¿¡æ¯] æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
            return

        for line in lines:
            try:
                record = json.loads(line)
                model_responses = record.get('model_response', {}).get('text', [])
                
                if not model_responses or not isinstance(model_responses, list):
                    continue

                # éå†å½“å‰é—®é¢˜çš„æ‰€æœ‰å›ç­”ï¼ˆè½®æ•°ï¼‰
                for run_index, response_text in enumerate(model_responses):
                    if isinstance(response_text, str):
                        token_ids = tokenizer.encode(response_text, add_special_tokens=False)
                        num_tokens = len(token_ids)
                        
                        responses_by_run[run_index].append(num_tokens)
                        total_token_count += num_tokens
                        total_responses += 1
            except (json.JSONDecodeError, KeyError):
                continue

    if total_responses == 0:
        print(f"  [ä¿¡æ¯] æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç­”æ¡ˆå­—æ®µ ('model_response' -> 'text')ï¼Œè·³è¿‡ã€‚")
        return

    # === ç¬¬äºŒæ­¥: è®¡ç®—æŒ‡æ ‡ ===
    num_runs = len(responses_by_run)
    
    # æŒ‡æ ‡1: æ€»å¹³å‡Token
    average_tokens_all = total_token_count / total_responses if total_responses > 0 else 0

    # æŒ‡æ ‡2 & 3: æœ€é«˜/æœ€ä½å¹³å‡Token
    highest_avg_token = 0
    lowest_avg_token = float('inf')

    if num_runs > 0:
        run_averages = []
        for i in range(num_runs):
            run_tokens = responses_by_run[i]
            # ç¡®ä¿åˆ†æ¯ä¸ä¸º0
            run_avg = sum(run_tokens) / len(run_tokens) if run_tokens else 0
            run_averages.append(run_avg)
        
        highest_avg_token = max(run_averages) if run_averages else 0
        lowest_avg_token = min(run_averages) if run_averages else 0
    else: # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„è½®æ¬¡ï¼Œæ‰€æœ‰æŒ‡æ ‡éƒ½ä¸€æ ·
        highest_avg_token = average_tokens_all
        lowest_avg_token = average_tokens_all


    # === ç¬¬ä¸‰æ­¥: æ‰“å°å’Œè®°å½•æ—¥å¿— ===
    print(f"  - æ€»é—®é¢˜æ•° (æ–‡ä»¶è¡Œæ•°): {num_questions}")
    print(f"  - æ¯ä¸ªé—®é¢˜çš„å›ç­”æ•° (è½®æ•°): {num_runs}")
    print(f"  - æ€»å›ç­”æ•°: {total_responses}")
    print(f"  - æ€»Tokenæ•°: {total_token_count}")
    print("-" * 30)
    print(f"  - 1. å¹³å‡Tokené•¿åº¦ (æ‰€æœ‰å›ç­”): {average_tokens_all:.2f}")
    if num_runs > 1:
        print(f"  - 2. æœ€é«˜å¹³å‡Tokené•¿åº¦ (å„è½®å¹³å‡ä¸­çš„æœ€å¤§å€¼): {highest_avg_token:.2f}")
        print(f"  - 3. æœ€ä½å¹³å‡Tokené•¿åº¦ (å„è½®å¹³å‡ä¸­çš„æœ€å°å€¼): {lowest_avg_token:.2f}")
    
    file_info = parse_info(data_path, args.tokenizer_path, args.base_model, args.is_base_model)
    if file_info:
        log_result_to_csv(args.log_file, file_info, 'avg_token_length', average_tokens_all)
        if num_runs > 1:
            log_result_to_csv(args.log_file, file_info, 'highest_avg_token_length', highest_avg_token)
            log_result_to_csv(args.log_file, file_info, 'lowest_avg_token_length', lowest_avg_token)
        print(f"  ğŸ“ æ‘˜è¦ç»“æœå·²è®°å½•è‡³: {args.log_file}")
    else:
        print(f"  âš ï¸  è­¦å‘Š: æœªèƒ½ä»æ–‡ä»¶å '{os.path.basename(data_path)}' ä¸­è§£æå…ƒæ•°æ®ã€‚")
    print("-" * 60)


# =================== ä¸»ç¨‹åºå…¥å£ (æ— å˜åŒ–) ===================
def main(args):
    try:
        print("æ­£åœ¨åŠ è½½Tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path, trust_remote_code=True, use_fast=False)
        print(f"âœ… Tokenizer from '{args.tokenizer_path}' loaded successfully.")
    except Exception as e:
        print(f"âŒ ä¸¥é‡é”™è¯¯: æ— æ³•åŠ è½½Tokenizerã€‚è¯·æ£€æŸ¥è·¯å¾„ '{args.tokenizer_path}'ã€‚é”™è¯¯: {e}")
        return

    files_to_process = []
    if os.path.exists(args.input_dir) and os.path.isdir(args.input_dir):
        for filename in os.listdir(args.input_dir):
            if filename.endswith("_results.jsonl"):
                files_to_process.append(os.path.join(args.input_dir, filename))

    if not files_to_process:
        print(f"åœ¨ç›®å½• '{args.input_dir}' ä¸­æœªæ‰¾åˆ°ä»»ä½• `_results.jsonl` æ–‡ä»¶ã€‚")
        return

    print(f"æ‰¾åˆ° {len(files_to_process)} ä¸ªæ–‡ä»¶éœ€è¦å¤„ç†ã€‚æ—¥å¿—å°†ä¿å­˜åˆ°: {args.log_file}")

    if args.clear_log:
        if os.path.exists(args.log_file):
            print(f"æ­£åœ¨æ¸…ç©ºæ—§çš„æ—¥å¿—æ–‡ä»¶: {args.log_file}")
            os.remove(args.log_file)
        
    for file_path in tqdm(files_to_process, desc="æ‰¹é‡è®¡ç®—Tokené•¿åº¦"):
        print(f"\nâ–¶ï¸  æ­£åœ¨å¤„ç†: {os.path.basename(file_path)}")
        calculate_and_log_token_stats(file_path, tokenizer, args)

    print("\nâœ… æ‰€æœ‰Tokené•¿åº¦è®¡ç®—å¤„ç†å®Œæ¯•ï¼")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="è®¡ç®—.jsonlæ–‡ä»¶çš„å¹³å‡/æœ€é«˜/æœ€ä½Tokené•¿åº¦ï¼Œå¹¶è®°å½•æ—¥å¿—ã€‚")
    
    parser.add_argument('--input_dir', type=str, default='/mnt/data/kw/tom/main_experiments/0907_1.5B_main/step_90/results', help="åŒ…å« _results.jsonl æ–‡ä»¶çš„æ ¹ç›®å½•ã€‚")
    parser.add_argument('--log_file', type=str, default='/mnt/data/kw/tom/main_experiments/0907_1.5B_main/step_90/results/token_len.csv', help="ç”¨äºè¿½åŠ æ‘˜è¦ç»“æœçš„ã€ç»Ÿä¸€ã€‘CSVæ—¥å¿—æ–‡ä»¶ã€‚")
    parser.add_argument('--tokenizer_path', type=str, default='/mnt/data/kw/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', help="Hugging Face tokenizerçš„è·¯å¾„ï¼ˆå°†ä»æ­¤è·¯å¾„æ¨æ–­æ¨¡å‹å’Œstepï¼‰ã€‚")
    parser.add_argument('--clear_log', action='store_true', help="åœ¨å¼€å§‹å‰æ¸…ç©ºæ—§çš„æ—¥å¿—æ–‡ä»¶ã€‚")
    
    parser.add_argument('--base_model', type=str, default='DeepSeek-R1-Distill-Qwen-7B', help="åŸºç¡€æ¨¡å‹çš„åç§°ï¼Œå°†ä½œä¸ºè®°å½•åœ¨CSVä¸­æ¨¡å‹åç§°çš„å‰ç¼€ã€‚")
    parser.add_argument('--is_base_model', action='store_true', help="ã€é‡è¦ã€‘å¦‚æœtokenizer_pathæŒ‡å‘çš„æ˜¯ä¸€ä¸ªbase modelï¼Œè¯·æ·»åŠ æ­¤æ ‡å¿—ã€‚")

    args = parser.parse_args()
    main(args)